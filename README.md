# FLAIR: Flexible Allocation of Informative Retrieval via Reinforcement Learning

FLAIR is a reinforcement learning framework for optimizing Retrieval-Augmented Generation (RAG) systems. It trains RL agents to dynamically select the optimal number of documents (top-k) for retrieval, improving both retrieval quality and computational efficiency.

## ğŸ¯ Overview

Traditional RAG systems use a fixed top-k value for document retrieval, which may not be optimal for all queries. FLAIR addresses this limitation by:

- **Adaptive Top-k Selection**: Uses reinforcement learning to learn query-specific optimal k values
- **Multi-Agent Support**: Implements DQN, Double DQN, and PPO algorithms
- **Efficient Retrieval**: FAISS-based similarity search with embedding models
- **Comprehensive Evaluation**: Combines recall@k, MRR@k, and efficiency metrics

## ğŸ—ï¸ Architecture

```
Query â†’ Embedding â†’ Agent â†’ Top-k Selection â†’ Retrieved Documents
  â†“         â†“         â†“           â†“              â†“
Dataset â†’ Retriever â†’ Environment â†’ Action â†’ Reward (Recall+MRR-Cost)
```

### Key Components

- **Environment** (`RAGTopKEnv`): Gym-based RL environment for top-k selection
- **Agents**: DQN, Double DQN, and PPO implementations
- **Retriever**: FAISS-based document retrieval with configurable embedding models
- **Trainer**: Unified training system with early stopping and performance monitoring

## ğŸ“‹ Requirements

- Python 3.8+
- PyTorch (CUDA support recommended)
- Transformers (Hugging Face)
- FAISS
- OpenAI Gym

## ğŸš€ Installation

1. Clone the repository:
```bash
git clone https://github.com/nqdhocai/FLAIR
cd FLAIR
```

2. Install dependencies:
```bash
pip install -r requirements.txt
```

## ğŸ“Š Dataset Format

FLAIR expects datasets in the following structure:
```
data/
  dataset_name/
    â”œâ”€â”€ corpus.csv          # Document collection (id, text)
    â”œâ”€â”€ question_train.csv  # Training queries (qid, question)
    â”œâ”€â”€ question_valid.csv  # Validation queries (qid, question)
    â”œâ”€â”€ question_test.csv   # Test queries (qid, question)
    â””â”€â”€ ground_truth.json   # Relevance labels {qid: [doc_ids]}
```

### Supported Datasets
- MS-MARCO (default)
- Custom datasets following the above format

## ğŸ® Usage

### Basic Training

```bash
python src/main.py \
    --embedding_model_id "sentence-transformers/all-MiniLM-L6-v2" \
    --agent_type "PPO" \
    --epochs 50 \
    --dataset_id "ms-marco" \
    --data_dir "./data"
```

### Command Line Arguments

| Argument | Type | Default | Description |
|----------|------|---------|-------------|
| `--embedding_model_id` | str | **Required** | Hugging Face model ID for embeddings |
| `--agent_type` | str | "PPO" | RL agent type: "DQN", "DoubleDQN", "PPO" |
| `--epochs` | int | 5 | Number of training epochs |
| `--dataset_id` | str | "ms-marco" | Dataset directory name |
| `--data_dir` | str | "../data" | Path to data directory |
| `--k_candidates` | list | [1,2,...,10] | Possible k values for top-k selection |

### Example Usage

```bash
# Train PPO agent with BERT embeddings
python src/main.py \
    --embedding_model_id "sentence-transformers/all-mpnet-base-v2" \
    --agent_type "PPO" \
    --epochs 100 \
    --k_candidates 1 2 3 4 5 6 7 8 9 10

# Train DQN agent for quick experiments
python src/main.py \
    --embedding_model_id "sentence-transformers/all-MiniLM-L6-v2" \
    --agent_type "DQN" \
    --epochs 25
```

## ğŸ§  RL Agents

### 1. DQN (Deep Q-Network)
- **Architecture**: 3-layer MLP with dropout
- **Features**: Experience replay, target network
- **Use Case**: Baseline discrete action selection

### 2. Double DQN
- **Architecture**: Same as DQN with double Q-learning
- **Features**: Reduces overestimation bias
- **Use Case**: More stable training than vanilla DQN

### 3. PPO (Proximal Policy Optimization)
- **Architecture**: Actor-Critic with shared layers
- **Features**: Policy gradient with clipping
- **Use Case**: Better sample efficiency and stability

## ğŸ¯ Reward Function

The reward function balances retrieval quality and efficiency:

```
R = Î±Â·recall@k + (1-Î±)Â·MRR@k - Î³Â·(k/K_max)Â·exp(âˆ’Î»Â·max(0, k âˆ’ kâˆ—))
```

With additional components:
- **Soft penalty**: Exponential decay for k > k*
- **No-overlap penalty**: Penalty when no relevant documents are retrieved
- **Normalization**: Rewards clamped to [-1, +1]

### Parameters
- `Î± = 0.6`: Balance between recall and MRR
- `Î³ = 0.1`: Cost coefficient for efficiency
- `Î²â‚€ = 0.5`: Adaptive penalty strength
- `no_overlap_penalty = 0.2`: Penalty for zero hits

## ğŸ“ˆ Monitoring & Evaluation

### Training Metrics
- **Average Reward**: Per-epoch reward progression
- **Loss**: Agent-specific loss curves
- **Early Stopping**: Automatic stopping on plateau

### Performance Analysis
```python
from src.trainer import plot_training_stats

# Plot training results
plot_training_stats(rewards, losses, agent_name="PPO")
```

## ğŸ”§ Customization

### Custom Embedding Models
```python
# Use any Hugging Face model
--embedding_model_id "microsoft/DialoGPT-medium"
--embedding_model_id "facebook/dpr-question_encoder-single-nq-base"
```

### Custom Reward Functions
Modify `compute_rewards()` in `src/enviroment.py`:
```python
def compute_rewards(self, retrieved_ids, gold_ids, **kwargs):
    # Implement custom reward logic
    return rewards
```

### Custom Agents
Extend base classes in `src/agents/`:
```python
class CustomAgent:
    def select_action(self, state):
        # Custom action selection
        pass
    
    def update(self):
        # Custom learning update
        pass
```

## ğŸ“ Project Structure

```
FLAIR/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main.py              # Main training script
â”‚   â”œâ”€â”€ enviroment.py        # RAG environment
â”‚   â”œâ”€â”€ trainer.py           # Unified training system
â”‚   â”œâ”€â”€ dataset.py           # Data loading utilities
â”‚   â”œâ”€â”€ agents/
â”‚   â”‚   â”œâ”€â”€ dqn.py          # DQN implementation
â”‚   â”‚   â”œâ”€â”€ ddqn.py         # Double DQN implementation
â”‚   â”‚   â””â”€â”€ ppo.py          # PPO implementation
â”‚   â””â”€â”€ retriever/
â”‚       â”œâ”€â”€ retriever.py    # FAISS-based retrieval
â”‚       â””â”€â”€ embedder.py     # Embedding model wrapper
â”œâ”€â”€ data/                    # Dataset directory
â”œâ”€â”€ requirements.txt         # Python dependencies
â””â”€â”€ README.md               # This file
```

## ğŸš§ Troubleshooting

### Common Issues

1. **CUDA Out of Memory**
   - Reduce batch size in embedding model
   - Use smaller embedding models
   - Process datasets in chunks

2. **Slow Training**
   - Enable CUDA if available
   - Use `cache_precompute=True` in environment
   - Reduce dataset size for experimentation

3. **Poor Convergence**
   - Adjust learning rates
   - Tune reward function parameters
   - Increase training epochs

### Performance Tips

- **GPU Usage**: Ensure PyTorch uses GPU for embeddings
- **Memory Optimization**: Use gradient checkpointing for large models
- **Batch Processing**: Optimize batch sizes based on available memory

## ğŸ“š Citation

If you use FLAIR in your research, please cite:

```bibtex
@article{flair2025,
  title={FLAIR: Flexible Allocation of Informative Retrieval via Reinforcement Learning},
  author={Your Name},
  journal={Your Journal},
  year={2025}
}
```

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Commit your changes
4. Push to the branch
5. Create a Pull Request

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- Hugging Face for transformer models
- FAISS team for efficient similarity search
- OpenAI Gym for RL environment framework
- PyTorch team for deep learning infrastructure